{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fa1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from types import SimpleNamespace   \n",
    "\n",
    "# config = {}\n",
    "# config['epochs'] = 10\n",
    "# config['batch_size'] = 8\n",
    "# config['lr'] = .00001\n",
    "# config['tokenizer_max_length'] = 512\n",
    "# config['file_min_words'] = 8\n",
    "# config['input_path'] = '../inputs/pubmed-targets-1'\n",
    "# config['output'] = './pharma-text-model'\n",
    "# config['tokenizer'] = 'allenai/biomed_roberta_base'\n",
    "# config['input_model'] = 'allenai/biomed_roberta_base'\n",
    "\n",
    "# conf = SimpleNamespace(**config)\n",
    "\n",
    "# with open(\"./config/pharma-text.json\", \"w\") as outfile:\n",
    "#     json.dump(config, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d860d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(batch_size=8, epochs=10, file_min_words=8, input_model='allenai/biomed_roberta_base', input_path='../inputs/pubmed-targets-1', lr=1e-05, output='./pharma-text-model', tokenizer='allenai/biomed_roberta_base', tokenizer_max_length=512)\n"
     ]
    }
   ],
   "source": [
    "## read config\n",
    "config = {}\n",
    "with open(\"./config/pharma-text.json\", \"r\") as infile:\n",
    "    config = json.load(infile)\n",
    "\n",
    "conf = SimpleNamespace(**config)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de02d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text dataset\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(conf.tokenizer)\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(conf.input_model)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path='text',\n",
    "    data_files=os.path.join(conf.input_path, '*')\n",
    ")\n",
    "\n",
    "if len(dataset['train']) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No text files were found under {conf.input_path}. Please update trainer/config/pharma-text.json to point to a directory with training data.\"\n",
    "    )\n",
    "\n",
    "# print(dir(dataset['train']))\n",
    "# print(dataset['train'].items())\n",
    "\n",
    "class dummy_data(transformers.TextDataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.inputs[index]\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "## filter out lines with fewer than some minimum words; they are likely not cohesive sentences\n",
    "def filter_data(d, min_words=conf.file_min_words):\n",
    "    return len(d['text'].split(' ')) >= min_words\n",
    "\n",
    "dataset = dataset.filter(filter_data)\n",
    "print(dataset['train'].data[:5])\n",
    "\n",
    "# dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "tokenized_data = tokenizer(dataset['train']['text'], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "print('Data size: ', len(tokenized_data['input_ids']))\n",
    "\n",
    "data_new = dummy_data(tokenized_data['input_ids'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "706c101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters for retraining on text dataset\n",
    "## Reference: https://towardsdatascience.com/transformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=conf.output,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=conf.epochs,\n",
    "    per_device_train_batch_size=conf.batch_size,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=data_new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(conf.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify model\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(conf.tokenizer)\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=conf.output,\n",
    "    tokenizer=tokenizer, \n",
    "    max_length=conf.tokenizer_max_length, \n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "features = feature_extraction([\n",
    "    \"This book is interesting. I would read it\", \n",
    "    \"I read this Jeffrey Archer book yesterday.\",\n",
    "    \"The latest batman movie is a waste of money.\"\n",
    "])\n",
    "embed1 = features[0][0][0]\n",
    "embed2 = features[1][0][0]\n",
    "embed3 = features[2][0][0]\n",
    "print('first two cosine: ', cosine(embed1, embed2))\n",
    "print('last two cosine: ', cosine(embed2, embed3))\n",
    "\n",
    "# for txt in dataset['train']['text']:\n",
    "features = feature_extraction(dataset['train']['text'][:5])\n",
    "print('Features: ', len(features))\n",
    "print('Features[0]', len(features[0]))\n",
    "print('Features[0][0]', len(features[0][0]))\n",
    "print('Features[0][0][0]', len(features[0][0][0]))\n",
    "embed1 = features[0][0][0]\n",
    "embed2 = features[1][0][0]\n",
    "embed3 = features[-1][0][0]\n",
    "print('first two cosine: ', cosine(embed1, embed2))\n",
    "print('last two cosine: ', cosine(embed2, embed3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf232742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1380it [00:00, 48824.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  1380\n",
      "Features[0] 1\n",
      "Features[0][0] 193\n",
      "Features[0][0][0] 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1380it [00:00, 25170.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output:  torch.Size([1380, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## save embeddings to disk\n",
    "from tqdm import tqdm\n",
    "from table_trainer_utils import write_csv\n",
    "import os\n",
    "import torch\n",
    "\n",
    "output_ids_path = '../features/ukopen-textids.list'\n",
    "output_embed_path = '../features/ukopen-textfeatures.pt'\n",
    "\n",
    "text_ids = []\n",
    "text_input = []\n",
    "\n",
    "for entry in tqdm(os.scandir(conf.input_path)):\n",
    "    if entry.is_dir():\n",
    "        continue\n",
    "    name = entry.name\n",
    "    text_ids.append(name)\n",
    "    with open(os.path.join(conf.input_path, name), 'r') as f:\n",
    "        text_input.append(f.readline())\n",
    "\n",
    "write_csv(output_ids_path, text_ids)\n",
    "\n",
    "features = feature_extraction(text_input)\n",
    "print('Features: ', len(features))\n",
    "print('Features[0]', len(features[0]))\n",
    "print('Features[0][0]', len(features[0][0]))\n",
    "print('Features[0][0][0]', len(features[0][0][0]))\n",
    "text_embeds = torch.empty(\n",
    "    (len(features), len(features[0][0][0])))\n",
    "\n",
    "for i, feature in tqdm(enumerate(features)):\n",
    "    text_embeds[i] = torch.tensor(feature[0][0])\n",
    "\n",
    "print('shape of output: ', text_embeds.shape)\n",
    "torch.save(text_embeds, output_embed_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}